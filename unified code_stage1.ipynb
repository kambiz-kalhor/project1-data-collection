{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing all the packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download a table from BacDive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i did it and the result is \"saved as stage1_step1_export_bacdive_iso_table before cleaning.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can access it in git_hub repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP TWO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read stage1_step1_export_bacdive_iso_table before cleaning.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stage1_step1_export_bacdive_iso_table before cleaning.csv\n",
    "bacDive = pd.read_csv(r\"C:\\Users\\kamy\\Desktop\\stage1_step1_export_bacdive_iso_table before cleaning.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the table # replacing NaN with no in three colums (Category 1, Category 2, Category 3)\n",
    "bacDive[\"Category 3\"].fillna(\"#no\", inplace = True)\n",
    "bacDive[\"Category 2\"].fillna(\"#no\", inplace = True)\n",
    "bacDive[\"Category 1\"].fillna(\"#no\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reason i used this code is to fill empty cells\n",
    "# this code means --> check the IDs, if the ID of two consecutive rows are the same than fill the second row with the cells of first row\n",
    "\n",
    "temporary_list =[]\n",
    "x = len(bacDive['ID'])-1\n",
    "for counter in range(0,x):\n",
    "    if bacDive.iloc[counter,0] == bacDive.iloc[counter+1,0]:\n",
    "        temporary_list.append(counter)\n",
    "        bacDive.iloc[counter+1,1] = bacDive.iloc[counter,1]\n",
    "        bacDive.iloc[counter+1,2] = bacDive.iloc[counter,2]\n",
    "        bacDive.iloc[counter+1,3] = bacDive.iloc[counter,3]\n",
    "        bacDive.iloc[counter+1,4] = bacDive.iloc[counter,4]\n",
    "        bacDive.iloc[counter+1,5] = bacDive.iloc[counter,5]\n",
    "        \n",
    "        # we do not need the next code because we want to maintain the Tag data\n",
    "        #bacDive.iloc[counter+1,7] = bacDive.iloc[counter,7] +  bacDive.iloc[counter+1,7]\n",
    "        #bacDive.iloc[counter+1,8] = bacDive.iloc[counter,8] +  bacDive.iloc[counter+1,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## we wont need this if we are going to maintain other Tags in future\n",
    "######## this code is written to remove all the rows without a specific Tag(here : #Environmental)\n",
    "\n",
    "\n",
    "#temporary_list =[]\n",
    "#for counter in range (0,len(bacDive.index)):\n",
    "    #if (bacDive.iloc[counter,6] != '#Environmental') :\n",
    "        #temporary_list.append(counter)\n",
    "\n",
    "######## we run this code at the end in order to keep all other category 1 tags \n",
    "#for i in temporary_list:\n",
    "    #bacDive = bacDive.drop([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a dataframe whithout any tags other than #Environmental  but there are still some redundency, there are some rows with the same Species name\n",
    "# here the goal is to merge rows with the same Species name\n",
    "\n",
    "\n",
    "temporary_list =[]\n",
    "x = len(bacDive['ID'])-1\n",
    "for counter in range(0,x):\n",
    "    if bacDive.iloc[counter,0] == bacDive.iloc[counter+1,0]:\n",
    "        temporary_list.append(counter)\n",
    "        bacDive.iloc[counter+1,7] = bacDive.iloc[counter,7] +  bacDive.iloc[counter+1,7]\n",
    "        bacDive.iloc[counter+1,8] = bacDive.iloc[counter,8] +  bacDive.iloc[counter+1,8]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we remove the duplicate row\n",
    "for i in temporary_list:\n",
    "    bacDive = bacDive.drop([i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP THREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEB SCRAPING from BacDive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to creat URLs using the BacDive IDs\n",
    "all_IDs = bacDive['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "producing links for web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ID_give_URL(ID):\n",
    "    url = 'https://bacdive.dsmz.de/strain/' + str(ID)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    return None\n",
    "#CODE = 200 means the url is availible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my regex to extract temperature data from BacDive\n",
    "# in future i should improve this regex # its a regex to find temperature Data\n",
    "my_regex = re.compile(\"(Ref\\.\\:.\\#\\d+)\\]\\<\\/a\\>\\<\\/td\\>\\s\\<td\\>\\<\\/td\\>\\s\\<td.class\\=\\\"border\\_rightfree\\ textalign\\_right\\\"\\>\\<\\/td\\>\\s\\<td.class\\=\\\"border\\_leftfree\\\"\\>(\\w+)\\<\\/td\\>\\s\\<td.class\\=\\\"border_leftfree textalign_center\\\"\\>(\\d{2}\\-\\d{2}|\\d{2}\\.\\d{1}|\\d{2})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_frame = pd.DataFrame()\n",
    "\n",
    "\n",
    "for ID in all_IDs:\n",
    "    url = get_ID_give_URL(ID)\n",
    "    html_doc = read_html(url)\n",
    "    \n",
    "    \n",
    "    if html_doc is None:\n",
    "        print(\"Something went wrong!!!  the following url seems to be wrong   ; \" , url)\n",
    "    \n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "    list_of_extracted_data = [ID]\n",
    "    \n",
    "    \n",
    "    #first step ==> extracting phylogeny data\n",
    "    tag = \"valigntop paddingright\"\n",
    "    data = soup.find_all(\"td\", class_= tag)\n",
    "    for td in data:\n",
    "        list_of_extracted_data.append(td.text)\n",
    "        if len(list_of_extracted_data) == 9:\n",
    "            break\n",
    "        \n",
    "    #second step ==> extracting temp data\n",
    "    soup = str(soup)\n",
    "    temperature_data = my_regex.findall(soup)\n",
    "    list_of_extracted_data = list_of_extracted_data +temperature_data\n",
    "    while len(list_of_extracted_data) != 16:\n",
    "        list_of_extracted_data.append(\"\")\n",
    "    my_data_frame[ID] = pd.Series(list_of_extracted_data)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "# transpose the dataframe\n",
    "my_data_frame = my_data_frame.T\n",
    "# naming columns\n",
    "my_data_frame = my_data_frame.rename(columns={0: 'ID',  1: 'Last LPSN update', 2: 'Domain', 3: 'Phylum', 4: 'Class', 5: 'Order', 6: 'Family', 7: 'Genus', 8: 'Species', 9: 'Full Scientific Name (PNU)', 10: 'temperature Ref 1', 11: 'temperature Ref 2', 12: 'temperature Ref 3', 13: 'temperature Ref 4', 14: 'temperature Ref 5', 15: 'temperature Ref 6', 16: 'temperature Ref 7'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP FOUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another cleaning and filling step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the cells and replacing \"NaN\" with \"#no\"\n",
    "# this will clean the dataframe for future use\n",
    "my_data_frame[\"temperature Ref 1\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 2\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 3\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 4\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 5\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 6\"].fillna(\"#no\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "until now, there are 0 species with no temperature data and you can see the list of IDs with no temp data in this: list_no_temp_species_ID\n"
     ]
    }
   ],
   "source": [
    "#### in order to know the species with no temperature data  ####\n",
    "list_index_no_temp = []\n",
    "list_no_temp_species_ID = []\n",
    "\n",
    "for counter in range (0,len(my_data_frame)):\n",
    "    if (my_data_frame.iloc[counter,10] == \"#no\"):\n",
    "        list_index_no_temp.append(counter)\n",
    "        list_no_temp_species_ID.append(my_data_frame.iloc[counter,0])\n",
    "        \n",
    "# give me an overview please\n",
    "print('until now, there are', str(len(list_no_temp_species_ID)) , 'species with no temperature data and you can see the list of IDs with no temp data in this: list_no_temp_species_ID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP FIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concat all the previous dataframes and producing an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making to dataframes look the same , so we can use concat()\n",
    "s = pd.Series(range(len(bacDive)))\n",
    "new_bac = new_bac.set_index([s])\n",
    "###########################################\n",
    "s = pd.Series(range(len(my_data_frame)))\n",
    "my_data_frame = my_data_frame.set_index([s])\n",
    "###########################################\n",
    "result = pd.concat([new_bac, my_data_frame], axis=1)\n",
    "###########################################\n",
    "result.to_csv(r'C:\\Users\\kamy\\Desktop\\final_output_of_stage1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
