{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for the dear user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please give me the string of path for the file \"stage1_step1_export_bacdive_iso_table before cleaning.csv\"\n",
    "# for example  --->   r'C:\\Users\\kamy\\Desktop\\stage1_step1_export_bacdive_iso_table before cleaning.csv'\n",
    "input_path = r'C:\\Users\\kamy\\Desktop\\INPUT.csv'\n",
    "\n",
    "\n",
    "# and again please give me the output path to save the final result\n",
    "output_path = r'C:\\Users\\kamy\\Desktop\\OUTPUT.csv'\n",
    "\n",
    "# this is another output which includes all_availible_seq_in_Bacdive\n",
    "second_output = r\"C:\\Users\\kamy\\Desktop\\all_availible_seq_in_Bacdive.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing all the packages we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main packages\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# using PubMed API\n",
    "from pymed import PubMed\n",
    "\n",
    "# using NCBI API\n",
    "from Bio import Entrez\n",
    "from Bio import SeqIO\n",
    "Entrez.email = 'kz.kalhor@gmail.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download a table from BacDive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i did it and the result is \"saved as stage1_step1_export_bacdive_iso_table before cleaning.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this file is our input and we rename it as 'INPUT.CSV'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can access it in git_hub repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP TWO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read stage1_step1_export_bacdive_iso_table before cleaning.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stage1_step1_export_bacdive_iso_table before cleaning.csv\n",
    "bacDive = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the table # replacing NaN with no in three colums (Category 1, Category 2, Category 3)\n",
    "bacDive[\"Category 3\"].fillna(\"#no\", inplace = True)\n",
    "bacDive[\"Category 2\"].fillna(\"#no\", inplace = True)\n",
    "bacDive[\"Category 1\"].fillna(\"#no\", inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the reason i used this code is to fill empty cells\n",
    "# this code means --> check the IDs, if the ID of two consecutive rows are the same than fill the second row with the cells of first row\n",
    "\n",
    "temporary_list =[]\n",
    "x = len(bacDive['ID'])-1\n",
    "for counter in range(0,x):\n",
    "    if bacDive.iloc[counter,0] == bacDive.iloc[counter+1,0]:\n",
    "        temporary_list.append(counter)\n",
    "        bacDive.iloc[counter+1,1] = bacDive.iloc[counter,1]\n",
    "        bacDive.iloc[counter+1,2] = bacDive.iloc[counter,2]\n",
    "        bacDive.iloc[counter+1,3] = bacDive.iloc[counter,3]\n",
    "        bacDive.iloc[counter+1,4] = bacDive.iloc[counter,4]\n",
    "        bacDive.iloc[counter+1,5] = bacDive.iloc[counter,5]\n",
    "        \n",
    "        # we do not need the next code because we want to maintain the Tag data\n",
    "        #bacDive.iloc[counter+1,7] = bacDive.iloc[counter,7] +  bacDive.iloc[counter+1,7]\n",
    "        #bacDive.iloc[counter+1,8] = bacDive.iloc[counter,8] +  bacDive.iloc[counter+1,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## we wont need this if we are going to maintain other Tags in future\n",
    "######## this code is written to remove all the rows without a specific Tag(here : #Environmental)\n",
    "\n",
    "\n",
    "#temporary_list =[]\n",
    "#for counter in range (0,len(bacDive.index)):\n",
    "    #if (bacDive.iloc[counter,6] != '#Environmental') :\n",
    "        #temporary_list.append(counter)\n",
    "\n",
    "######## we run this code at the end in order to keep all other category 1 tags \n",
    "#for i in temporary_list:\n",
    "    #bacDive = bacDive.drop([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have a dataframe whithout any tags other than #Environmental  but there are still some redundency, there are some rows with the same Species name\n",
    "# here the goal is to merge rows with the same Species name\n",
    "\n",
    "\n",
    "temporary_list =[]\n",
    "x = len(bacDive['ID'])-1\n",
    "for counter in range(0,x):\n",
    "    if bacDive.iloc[counter,0] == bacDive.iloc[counter+1,0]:\n",
    "        temporary_list.append(counter)\n",
    "        bacDive.iloc[counter+1,7] = bacDive.iloc[counter,7] +  bacDive.iloc[counter+1,7]\n",
    "        bacDive.iloc[counter+1,8] = bacDive.iloc[counter,8] +  bacDive.iloc[counter+1,8]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now we remove the duplicate row (consecutive duplicated rows only)\n",
    "for i in temporary_list:\n",
    "    bacDive = bacDive.drop([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the repeated species \n",
    "list_of_Indexes = []\n",
    "\n",
    "new_list_of_IDs =[]\n",
    "for i in bacDive.index:\n",
    "    if bacDive.loc[i,'ID'] not in new_list_of_IDs:\n",
    "        new_list_of_IDs.append(bacDive.loc[i,'ID'])\n",
    "    else:\n",
    "        list_of_Indexes.append(i)\n",
    "        \n",
    "        \n",
    "bacDive = bacDive.drop(list_of_Indexes)\n",
    "\n",
    "#################################################################\n",
    "# i found a wired flaw in BacDive database : here in this code we remove the rows with the same species name \n",
    "# the result is a dataframe with 8681 row\n",
    "# now if we remove the rows with the same ID, the result will be a dataframe with 18040 rows\n",
    "# this means that there are some species with the same name but with different ID\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP THREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WEB SCRAPING from BacDive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to creat URLs using the BacDive IDs\n",
    "all_IDs = bacDive['ID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "producing links for web scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ID_give_URL(ID):\n",
    "    url = 'https://bacdive.dsmz.de/strain/' + str(ID)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_html(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    return None\n",
    "#CODE = 200 means the url is availible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my regex to extract temperature data from BacDive\n",
    "# in future i should improve this regex \n",
    "my_regex_temperature = re.compile(\"(Ref\\.\\:.\\#\\d+)\\]\\<\\/a\\>\\<\\/td\\>\\s\\<td\\>\\<\\/td\\>\\s\\<td.class\\=\\\"border\\_rightfree\\ textalign\\_right\\\"\\>\\<\\/td\\>\\s\\<td.class\\=\\\"border\\_leftfree\\\"\\>(\\w+)\\<\\/td\\>\\s\\<td.class\\=\\\"border_leftfree textalign_center\\\"\\>(\\d{2}\\-\\d{2}|\\d{2}\\.\\d{1}|\\d{2})\")\n",
    "\n",
    "# my regex to extract pH data from BacDive\n",
    "# in future i should improve this regex \n",
    "x=str('(Ref\\.\\:.\\#\\d+)\\]\\<\\/a\\>\\<\\/td\\>\\\\n\\<td\\>\\<\\/td\\>\\\\n\\<td\\sclass=\\\"border\\_rightfree\\stextalign\\_right\\\"\\>\\<\\/td\\>\\\\n\\<td\\sclass=\\\"border\\_leftfree\\\"\\>(\\w+)\\<\\/td\\>\\\\n\\<td\\sclass=\\\"valigntop\\sborder\\stextalign\\_center\\\"\\>(\\d+\\.\\d+\\-\\d\\.\\d+)')\n",
    "my_regex_pH = re.compile(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_frame = pd.DataFrame()\n",
    "\n",
    "\n",
    "for ID in all_IDs:\n",
    "    url = get_ID_give_URL(ID)\n",
    "    html_doc = read_html(url)\n",
    "    \n",
    "    \n",
    "    if html_doc is None:\n",
    "        print(\"Something went wrong!!!  the following url seems to be wrong   ; \" , url)\n",
    "    \n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "    list_of_extracted_data = [ID]\n",
    "    \n",
    "    \n",
    "    #first step ==> extracting phylogeny data\n",
    "    tag = \"valigntop paddingright\"\n",
    "    data = soup.find_all(\"td\", class_= tag)\n",
    "    for td in data:\n",
    "        list_of_extracted_data.append(td.text)\n",
    "        if len(list_of_extracted_data) == 9:\n",
    "            break\n",
    "        \n",
    "    #second step ==> extracting temp data\n",
    "    soup = str(soup)\n",
    "    temperature_data = my_regex_temperature.findall(soup)\n",
    "    list_of_extracted_data = list_of_extracted_data +temperature_data\n",
    "    while len(list_of_extracted_data) != 16:\n",
    "        list_of_extracted_data.append(\"\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    #third step ==> extracting phylogeny data\n",
    "    \n",
    "    soup = str(soup)\n",
    "    pH_data = my_regex_pH.findall(soup)\n",
    "    list_of_extracted_data = list_of_extracted_data + pH_data\n",
    "    while len(list_of_extracted_data) != 22:\n",
    "        list_of_extracted_data.append(\"\")\n",
    "    my_data_frame[ID] = pd.Series(list_of_extracted_data)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "# transpose the dataframe\n",
    "my_data_frame = my_data_frame.T\n",
    "# naming columns\n",
    "my_data_frame = my_data_frame.rename(columns={0: 'ID',  1: 'Last LPSN update', 2: 'Domain', 3: 'Phylum', 4: 'Class', 5: 'Order', 6: 'Family', 7: 'Genus', 8: 'species', 9: 'temperature Ref 1', 10: 'temperature Ref 2', 11: 'temperature Ref 3', 12: 'temperature Ref 4', 13: 'temperature Ref 5', 14: 'temperature Ref 6', 15: 'temperature Ref 7', 16: 'pH 1', 17: 'pH 2', 18: 'pH 3', 19: 'pH 4', 20: 'pH 5', 21: 'pH 6'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP FOUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "another cleaning and filling step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the cells and replacing \"NaN\" with \"#no\"\n",
    "# this will clean the dataframe for future use\n",
    "my_data_frame[\"temperature Ref 1\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 2\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 3\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 4\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 5\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 6\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"temperature Ref 7\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"pH 1\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"pH 2\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"pH 3\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"pH 4\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"pH 5\"].fillna(\"#no\", inplace = True)\n",
    "my_data_frame[\"pH 6\"].fillna(\"#no\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "until now, there are 0 species with no temperature data and you can see the list of IDs with no temp data in this: list_no_temp_species_ID\n"
     ]
    }
   ],
   "source": [
    "#### in order to know the species with no temperature data  ####\n",
    "list_index_no_temp = []\n",
    "list_no_temp_species_ID = []\n",
    "\n",
    "for counter in range (0,len(my_data_frame)):\n",
    "    if (my_data_frame.iloc[counter,10] == \"#no\"):\n",
    "        list_index_no_temp.append(counter)\n",
    "        list_no_temp_species_ID.append(my_data_frame.iloc[counter,0])\n",
    "        \n",
    "# give me an overview please\n",
    "print('until now, there are', str(len(list_no_temp_species_ID)) , 'species with no temperature data and you can see the list of IDs with no temp data in this: list_no_temp_species_ID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP FIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "concat all the previous dataframes and producing an output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making to dataframes look the same , so we can use concat()\n",
    "s = pd.Series(range(len(bacDive)))\n",
    "bacDive = bacDive.set_index([s])\n",
    "###########################################\n",
    "s = pd.Series(range(len(my_data_frame)))\n",
    "my_data_frame = my_data_frame.set_index([s])\n",
    "###########################################\n",
    "result = pd.concat([bacDive, my_data_frame], axis=1)\n",
    "\n",
    "###########################################\n",
    "#result.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP SIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this step we want to collect some data using PubMed API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating queries to download the species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_species = result['Species']\n",
    "list_of_IDs = result['ID']\n",
    "\n",
    "#this code is because we have to ID columns in the result dataframe --> i should remove it in the future\n",
    "list_of_IDs = list_of_IDs.iloc[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we dont need this step here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the repeated species  (no need)\n",
    "#new_list_of_species =[]\n",
    "#for i in list_of_species:\n",
    "    #if i not in new_list_of_species:\n",
    "        #new_list_of_species.append(i)\n",
    "        \n",
    "#list_of_species = new_list_of_species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# our regexes to extract pH and optimum pH (bad regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## regexes to find pH  #############################################################\n",
    "\n",
    "regex1 = r'[^p].[^i][^m][^u][^m].pH (\\d\\d?)[^\\.][^\\d]'                                          # pH 4\n",
    "regex2 = r'[^p].[^i][^m][^u][^m].pH of (\\d\\d?\\.?\\d?\\d?)\\-?(\\d?\\d?\\.?\\d?\\d?) '                   # pH of 7-11   #pH of 7\n",
    "regex3 = r'[^p].[^i][^m][^u][^m].pH (\\d\\d?\\.\\d?\\d?)'                                            # pH 4.54 \n",
    "regex4 = r'[^p].[^i][^m][^u][^m].pH (\\d\\d?\\.?\\d?\\d?) to (\\d\\d?\\.?\\d?\\d?)'                       # pH 4.54 to 5.32\n",
    "regex5 = r'[^p].[^i][^m][^u][^m].pH (\\d\\d?\\.?\\d?\\d?) and (\\d\\d?\\.?\\d?\\d?)'                      # pH 4.54 and 5.32\n",
    "regex6 = r'[^p].[^i][^m][^u][^m].pH (\\d\\d?\\.?\\d?\\d?) and pH (\\d\\d?\\.?\\d?\\d?)'                   # pH 3.23 and pH 6.5\n",
    "regex7 = r'[^p].[^i][^m][^u][^m].pH (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'                         # pH 5.33-4.23\n",
    "regex8 = r'[^p].[^i][^m][^u][^m].pH of the medium was adjusted to (\\d\\d?\\.?\\d?\\d?)'             # pH of the medium was adjusted to 6.4\n",
    "regex9 = r'[^p].[^i][^m][^u][^m].pH range.+?from (\\d\\d?\\.?\\d?\\d?) to (\\d\\d?\\.?\\d?\\d?)'          # pH range for growth at 70 \"C was from 4.4 to 7.5\n",
    "regex10 = r'[^p].[^i][^m][^u][^m].pH range.+?from (\\d\\d?\\.?\\d?\\d?) to pH (\\d\\d?\\.?\\d?\\d?)'      # pH range for growth at 70 \"C was from 4.4 to pH 7.5\n",
    "regex11 = r'[^p].[^i][^m][^u][^m].pH range.+?from (\\d\\d?\\.?\\d?\\d?) to pH (\\d\\d?\\.?\\d?\\d?)'      # pH range 6-9\n",
    "regex12 = r'from pH (\\d\\d?\\.?\\d?\\d?) to (\\d\\d?\\.?\\d?\\d?)'                                       # from pH 5?5 to 7?0\n",
    "regex13 = r'from pH (\\d\\d?\\.?\\d?\\d?) to pH (\\d\\d?\\.?\\d?\\d?)'                                    # from pH 5?5 to pH 7?0\n",
    "regex14 = r'[^p].[^i][^m][^u][^m].pH from (\\d\\d?\\.?\\d?\\d?) to pH (\\d\\d?\\.?\\d?\\d?)'              # pH from 5?5 to pH 7?0\n",
    "regex15 = r'[^p].[^i][^m][^u][^m].pH from (\\d\\d?\\.?\\d?\\d?) to (\\d\\d?\\.?\\d?\\d?)'                 # pH from 5?5 to 7?0\n",
    "regex16 = r'[^p].[^i][^m][^u][^m].pH values of (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'              # pH values of 4.8–5.8\n",
    "regex17 = r'[^p].[^i][^m][^u][^m].pH values.+?(\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'               # pH values (4.8–5.8)\n",
    "regex18 = r'[^p].[^i][^m][^u][^m].pH values of (\\d\\d?\\.?\\d?\\d?)'                                # pH values of 4.8\n",
    "regex19 = r'pH values ranging from (\\d\\d?\\.?\\d?\\d?)[ toand\\-or]+?(\\d\\d?\\.?\\d?\\d?)'              # pH values ranging from 7.5 to 9.0\n",
    "regex20 = r'[^p].[^i][^m][^u][^m].pH around (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'                 # pH around 6.0–6.5\n",
    "regex21 = r'[^p].[^i][^m][^u][^m].pH around (\\d\\d?\\.?\\d?\\d?)'                                   # pH around 6.0\n",
    "regex22 = r'[^p].[^i][^m][^u][^m].pH growth range (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'           # pH growth range 3.5-6.4\n",
    "regex23 = r'[^p].[^i][^m][^u][^m].pH range for growth[is was for of]+?(\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'    # the pH range for growth of 2.0-6.0  # the pH range for growth is 2.0-6.0   # the pH range for growth was 2.0-6.0\n",
    "regex24 = r'[^p].[^i][^m][^u][^m].pH between (\\d\\d?\\.?\\d?\\d?) and (\\d?\\d?\\.?\\d?\\d?)'                         # pH between 7.5 and 10.5 (optimum 8.8-9)\n",
    "regex25 = r'[^m][^u][^m] pH\\(\\d\\d.+?\\).+?(\\d\\d?\\.?\\d?\\d?)[ -andorto]+?(\\d?\\d[\\.]\\d?\\d?)'                     #The pH(60 degrees C) range for growth was 4.0-8.0\n",
    "\n",
    "\n",
    "######################################## regexes to find optimum pH  #############################################################\n",
    "\n",
    "\n",
    "reggex1 = r'(optimum) pH (\\d\\d?\\.?\\d?\\d?)'                                  #optimum pH 5.6\n",
    "reggex2 = r'(optimum) pH (\\d\\d?\\.?\\d?\\d?)'                                  #optimum pH 5.6\n",
    "reggex3 = r'(optimum) pH (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'                #optimum pH 5.6-6.5\n",
    "reggex4 = r'(optimum) pH (\\d\\d?\\.?\\d?\\d?) or (\\d\\d?\\.?\\d?\\d?)'              #optimum pH 5.6 or 6.5\n",
    "reggex5 = r'(optimum) pH (\\d\\d?\\.?\\d?\\d?) and (\\d\\d?\\.?\\d?\\d?)'             #optimum pH 5.6 and 6.5\n",
    "reggex6 = r'(optimum) at pH (\\d\\d?\\.?\\d?\\d?)'                               #optimum at pH 7.25\n",
    "reggex7 = r'(optimum), pH (\\d\\d?\\.?\\d?\\d?)\\-?(\\d?\\d?\\.?\\d?\\d?)'             #optimum, pH 8.0-9.0\n",
    "reggex8 = r'(optimum) pH was (\\d\\d?\\.?\\d?\\d?)'                              #optimum pH was 2.3\n",
    "reggex9 = r'(optimum) pH was (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d)'             #optimum pH was 2.3-9.2\n",
    "reggex10 = r'growing (optimally) at pH (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'  #growing optimally at pH 3.5-4.0\n",
    "reggex11 = r'ptimum pH around (\\d\\d?\\.?\\d?\\d?)'                             #optimum pH around 6.0\n",
    "reggex12 = r'ptimum pH of the medium was adjusted to (\\d\\d?\\.?\\d?\\d?)'                        #optimum pH of the medium was adjusted to 6.4\n",
    "reggex13 = r'ptimum pH range.+?from (\\d\\d?\\.?\\d?\\d?) to (\\d\\d?\\.?\\d?\\d?)'                     #optimum pH range for growth at 70 \"C was from 4.4 to 7.5\n",
    "reggex14 = r'ptimum pH range.+?from (\\d\\d?\\.?\\d?\\d?) to pH (\\d\\d?\\.?\\d?\\d?)'                  #optimum pH range for growth at 70 \"C was from 4.4 to pH 7.5\n",
    "reggex15 = r'[^p].[^i][^m][^u][^m].pH range (\\d\\d?\\.?\\d?\\d?)[ toand\\-or]+?(\\d\\d?\\.?\\d?\\d?)'   #from optimum pH 5?5 to 7?0\n",
    "reggex16 = r'from optimum pH (\\d\\d?\\.?\\d?\\d?) to (\\d\\d?\\.?\\d?\\d?)'                            #from optimum pH 5?5 to 7?0\n",
    "reggex17 = r'from optimum pH (\\d\\d?\\.?\\d?\\d?) to pH (\\d\\d?\\.?\\d?\\d?)'                         #from optimum pH 5?5 to pH 7?0\n",
    "reggex18 = r'ptimum pH from (\\d\\d?\\.?\\d?\\d?) to pH (\\d\\d?\\.?\\d?\\d?)'                          #optimum pH from 5?5 to pH 7?0\n",
    "reggex19 = r'ptimum pH values of (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'                          #optimum pH values of 4.8–5.8\n",
    "reggex20 = r'ptimum pH values.+?(\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'                           #optimum pH values (4.8–5.8)\n",
    "reggex21 = r'ptimum pH values of (\\d\\d?\\.?\\d?\\d?)'                                            #optimum pH values of 4.8 \n",
    "reggex22 = r'ptimum pH around (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'                             #optimum pH around 6.0–6.5\n",
    "reggex23 = r'ptimum pH around (\\d\\d?\\.?\\d?\\d?)'                                               #optimum pH around 6.0\n",
    "reggex24 = r'ptimum pH growth range (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'                       #optimum pH growth range 3.5-6.4\n",
    "reggex25 = r'ptimum pH range for growth of (\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d?)'                #optimum the pH range for growth of 2.0-6.0\n",
    "reggex26 = r'pH growth range \\d\\d?\\.?\\d?\\d?\\-\\d\\d?\\.?\\d?\\d?.?.?.?.?.?(optimum).*?(\\d\\d?\\.?\\d?\\d?)\\-(\\d\\d?\\.?\\d?\\d)'            #(pH growth range 3.5-6.4; optimum, 4.0-4.5) than strain KA1(T)\n",
    "reggex27 = r'pH range for growth of \\d\\d?\\.?\\d?\\d?\\-\\d\\d?\\.?\\d?\\d \\(with an (optimum) at (\\d\\d?\\.?\\d?\\d?)\\-?(\\d?\\d?\\.?\\d?\\d?)' #the pH range for growth of 2.0-6.0 (with an optimum at 3.8)\n",
    "reggex28 = r'The (optimum) growth temperature and pH were found to be \\d\\d?\\.?\\d?\\d?-\\d\\d?\\.?\\d?\\d?.+?and (\\d\\d?\\.?\\d?\\d?)-(\\d\\d?\\.?\\d?\\d?).+?respectively'\n",
    "#The optimum growth temperature and pH were found to be 25-30 °C and 5.0-7.0, respectively\n",
    "reggex29 = r'(optimal) pH of (\\d\\d?\\.?\\d?\\d?)-(\\d\\d?\\.?\\d?\\d?)'      #optimal pH of 6-8\n",
    "reggex30 = r'pH between \\d\\d?\\.?\\d?\\d? and \\d?\\d?\\.?\\d?\\d? \\((optimum) (\\d\\d?\\.?\\d?\\d?)\\-(\\d?\\d?\\.?\\d?\\d?)'\n",
    "reggex31 = r'(optimum) pH for growth was (\\d\\d?\\.?\\d?\\d?)'           #optimum pH for growth was 9.5\n",
    "reggex32 = r'(optimum) pH for growth was (\\d\\d?\\.?\\d?\\d?)[ toand\\-or]+?(\\d\\d?\\.?\\d?\\d?)'\n",
    "#optimum pH for growth was 9.5 to 3.43   #optimum pH for growth was 9.5 and 8.4    #optimum pH for growth was 9.5-3.4    #optimum pH for growth was 9.5 or 3.2\n",
    "reggex33 = r'(optimum).pH was between (\\d\\d?\\.?\\d?\\d?)[ toand\\-or]+?(\\d\\d?\\.?\\d?\\d?)'\n",
    "reggex34 = r'pH range for growth[is was for of]+?\\d\\d?\\.?\\d?\\d?\\-\\d\\d?\\.?\\d?\\d?.+?(optimum) (\\d\\d?\\.?\\d?\\d?)\\-?(\\d?\\d?\\.?\\d?\\d?)'\n",
    "reggex35 = r'(optimum) pH[ isbetween,;:was]+(\\d\\d?\\.?\\d?\\d?)[ \\-andto]+(\\d\\d?\\.?\\d?\\d?)'    #optimum pH between 7.0 and 8.5\n",
    "reggex36 = r'(optimum) pH[ isbetween,;:was]+(\\d\\d?\\.?\\d?\\d?)'\n",
    "reggex37 = r'temperature and pH for (optimum) growth were \\d+[^\\d]+(\\d\\d?\\.?\\d?\\d?)' #The temperature and pH for optimum growth were 30 °C and 7.5\n",
    "\n",
    "\n",
    "\n",
    "#####################################  make a list of regexes   #################################################\n",
    "bad_regexes = [regex1,regex2,regex3,regex4,regex5,regex6,regex7,regex8,regex9,regex10,regex11,regex12,regex13,regex14,regex15,regex16,regex17,regex18,regex19,regex20,regex21,regex22,regex23,regex24,regex25,      reggex1,reggex2,reggex3,reggex4,reggex5,reggex6,reggex7,reggex8,reggex9,reggex10,reggex11,reggex12,reggex13,reggex14,reggex15,reggex16,reggex17,reggex18,reggex19,reggex20,reggex21,reggex22,reggex23,reggex24,reggex25,reggex26,reggex27,reggex28,reggex29,reggex30,reggex31,reggex32,reggex33,reggex34,reggex35,reggex36,reggex37]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more advanced kind of regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_regex_for_pH =r'(?: from | range |)(?:(?:(optimally)|(optimum)|(optimal)|(optima)|growth|(?#next step is because we dont want and/to/or before our pH))(?: |, | at |)pH(?:s|)(?: growth|)(?: (optimal)| (optimum)| (optima)|(?: |)\\(\\d.*?(?:C|c).*?\\) ?|(?: |)\\(\\d.*?degrees.*?\\) ?| range| ranged| |))(?:(?:(?: for.+?|)|(?: growth|)(?:| range(?:| for growth)| values))(?:(?: was| were| is| are)(?:.{0,30}?)|)(?:| of| at| approximately| around| between| from| ranging from)| of the medium was adjusted to)(?#from here its about digits)(?:(?: |\\(| \\(|)(?#here is the first pH)((?:[1][01234]|[0-9])(?:\\.\\d|\\.\\d\\d|))(?:(?#here is the seperators)(?: |–|\\-| to | and | or | and pH | or pH | to pH |\\-pH | and pH| or pH| to pH|\\-pH)(?#here is the second pH)((?:[1][01234]|[0-9])(?:\\.\\d|\\.\\d\\d|))|))(?#end of digits)(?#what comes after pH digits)(?:.{0,20}(optimum)(?#from here is the optimum that sometimes comes at the end of the main part, so from now the main sentence is finished)(?:(?#from here its about digits)(?:(?#here is the first pH)(?:.{0,10}?((?:[1][01234]|[0-9])(?:\\.\\d|\\.\\d\\d|)))(?:(?#here is the seperators)(?: |–|\\-| to | and | or | and pH | or pH | to pH |\\-pH )(?#here is the second pH)((?:[1][01234]|[0-9])(?:\\.\\d|\\.\\d\\d|))|)))|)(?#end of digits)(?#what comes after pH digits)(?=(?:\\)|,|;|:| |\\.))(?![c|C|°|d|%]| [c|C|°|d|%]|  [c|C|°|d|%])'\n",
    "other_regexes = r'(neutral) pH|(neutral to alkaline) pH'\n",
    "\n",
    "advanced_regexes = [general_regex_for_pH, other_regexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last resort regex for the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this needs to be checked\n",
    "last_resort =r'(?:(opti).{0,40}|)pH.{0,80}?(?:(optimally)|(optimum)|(optimal)|(optima)|).{0,20}?\\D((?:[1][01234]|[0-9])(?:\\.\\d|\\.\\d\\d|))(?:(?#here is the seperators)(?: |–|\\-| to | and | or | and pH | or pH | to pH |\\-pH )(?#here is the second pH)((?:[1][01234]|[0-9])(?:\\.\\d|\\.\\d\\d|))|)(?=(?:\\)|,|;|:| |\\.\\D))(?![c|C|°|d|%]| [c|C|°|d|%]|  [c|C|°|d|%])'\n",
    "\n",
    "last_regexes = [last_resort]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first regex for temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_version = r'(?: |\\()(\\d\\d|\\d)(?:(?:(?:c|C|°|degrees)|.(?:c|C|°|degrees)|..(?:c|C|°|degrees))|)(?:(?: |–|\\-| to | and | or | and pH | or temperature | to temperature | temperature )(\\d\\d|\\d)|)(?:(?:c|C|°|degrees)|.(?:c|C|°|degrees)|..(?:c|C|°|degrees))(?!%|.%|..%|\\d|\\w)'\n",
    "first_temperature_regex = [first_version]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# my def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this def gets an species name and gives a string(query) which we can later use to search pubmed\n",
    "# here we want \"nov\" to be in our title ----> nov means\n",
    "# we want pH to be in either Title or Abstract\n",
    "\n",
    "def make_pubmed_advance_search_query(species_name):\n",
    "    query = '((' + str(species_name) + '[Title]) AND (nov[Title])) AND (pH[Title/Abstract])'\n",
    "    \n",
    "    return (query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this def gets a string(query include species name) and gives an abstract using pubmed API\n",
    "\n",
    "def get_abstract_from_pubmed(query):\n",
    "    \n",
    "    # Create a PubMed object that GraphQL can use to query\n",
    "    # Note that the parameters are not required but kindly requested by PubMed Central\n",
    "    # https://www.ncbi.nlm.nih.gov/pmc/tools/developers/\n",
    "    pubmed = PubMed(tool=\"MyTool\", email=\"kz.kalhor@gmail.com\")\n",
    "\n",
    "    # Execute the query against the API\n",
    "    results = pubmed.query(query, max_results=500)\n",
    "\n",
    "    # Loop over the retrieved articles\n",
    "    for article in results:\n",
    "\n",
    "        # Extract and format information from the article\n",
    "        article_id = article.pubmed_id\n",
    "        title = article.title\n",
    "        if article.keywords:\n",
    "            if None in article.keywords:\n",
    "                article.keywords.remove(None)\n",
    "            keywords = '\", \"'.join(article.keywords)\n",
    "        publication_date = article.publication_date\n",
    "        abstract = article.abstract\n",
    "\n",
    "\n",
    "        # # make a file for the next step\n",
    "        result_of_search = (\n",
    "            f'{article_id} - {publication_date} - {title}\\n \\n{abstract}\\n'\n",
    "        )\n",
    "        \n",
    "        return(result_of_search)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this function I remove unicode characters\n",
    "# in future i had to learn a better way to remove this characters ---> finally i made it better at 30 march 2021\n",
    "# remmember that in future you can add other characters you dont want to be removed\n",
    "\n",
    "def get_abstract_make_changes(abstract):\n",
    "    abstract = re.sub(\"[^a-zA-Z0-9 ,;%:°.\\-\\(\\)\\–\\_]+\", \"\" ,abstract)\n",
    "    return(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this def gets an Abstract and gives an important sentence which includes pH\n",
    "# this step is not neccessary but i need it because i want to check the results using this sentences\n",
    "\n",
    "def find_sentence_with_pH_data(abstract):\n",
    "    the_sentence_about_pH = None\n",
    "    # a is where the pH is located in the text\n",
    "    if 'pH' in abstract:\n",
    "        a = abstract.index('pH')\n",
    "        # here we selecte the surrounding text\n",
    "        s = abstract[a-300:a+300]\n",
    "        # spliting the right sentence\n",
    "        x = s.split(\". \")\n",
    "        for i in range(0,len(x)):\n",
    "            if 'pH' in x[i]:\n",
    "                the_sentence_about_pH = x[i].replace('\\u2009', ' ')  # this code removes a unicode problem\n",
    "\n",
    "        return (the_sentence_about_pH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this def gets an Abstract and gives an important sentence which includes temperature\n",
    "# this step is not neccessary but i need it because i want to check the results using this sentences\n",
    "\n",
    "def find_sentence_with_temerature_data(abstract):\n",
    "    the_sentence_about_temerature = None\n",
    "    # a is where the temerature is located in the text\n",
    "    if 'temperature' in abstract:\n",
    "        a = abstract.index('temperature')\n",
    "        # here we selecte the surrounding text\n",
    "        s = abstract[a-300:a+300]\n",
    "        # spliting the right sentence\n",
    "        x = s.split(\". \")\n",
    "        for i in range(0,len(x)):\n",
    "            if 'temperature' in x[i]:\n",
    "                the_sentence_about_temerature = x[i].replace('\\u2009', ' ')  # this code removes a unicode problem\n",
    "\n",
    "        return (the_sentence_about_temerature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final def to search for whatever information we want from a string\n",
    "# the string can be either the pH sentence or the complete abstract\n",
    "# it gets the 'pH_sentence' and gives a list of what_regex_find\n",
    "\n",
    "def get_sentence_give_pH_data(pH_sentence):\n",
    "    regexes = regexes_pH\n",
    "    pH_sentence = str(pH_sentence)    #I did this because of an error\n",
    "    what_regex_found = []\n",
    "    for regex in regexes:\n",
    "        \n",
    "        \n",
    "        what_is_found = re.findall(regex, pH_sentence)\n",
    "        if what_is_found != []:\n",
    "            what_regex_found.append(what_is_found)\n",
    "        \n",
    "    return (what_regex_found)\n",
    "\n",
    "# this code needs a list of regexes\n",
    "# it gets a sentence as an imput\n",
    "# the output is a list of what regex found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the final def to search for whatever information we want from a string\n",
    "# the string can be either temerature sentence or the complete abstract\n",
    "# it gets the 'the_sentence_about_temerature' and gives a list of what_regex_find\n",
    "\n",
    "def get_sentence_give_temperature_data(the_sentence_about_temerature):\n",
    "    regexes = first_temperature_regex\n",
    "    the_sentence_about_temerature = str(the_sentence_about_temerature)    #I did this because of an error\n",
    "    what_regex_found = []\n",
    "    for regex in regexes:\n",
    "        \n",
    "        \n",
    "        what_is_found = re.findall(regex, the_sentence_about_temerature)\n",
    "        if what_is_found != []:\n",
    "            what_regex_found.append(what_is_found)\n",
    "        \n",
    "    return (what_regex_found)\n",
    "\n",
    "# this code needs a list of regexes\n",
    "# it gets a sentence as an imput\n",
    "# the output is a list of what regex found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# assemble all the pervious codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first tell me which regex do you want to use??\n",
    "\n",
    "#regexes_pH = bad_regexes\n",
    "regexes_pH = advanced_regexes\n",
    "#regexes_pH = last_regexes\n",
    "\n",
    "\n",
    "regexes_temperature = first_temperature_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a data frame for final storage\n",
    "df = pd.DataFrame(columns = ['ID', 'species_name' ,  'query' ,  'abstract' ,   'pH_sentence' , 'what_regexss_found_pH', 'the_sentence_about_temerature','what_regexss_found_temperature'])\n",
    "# this list help me to find specied with no record\n",
    "list_of_species_with_no_record = []\n",
    "\n",
    "for i in range(0,len(list_of_species)):\n",
    "    ID = list_of_IDs[i]\n",
    "    species_name = list_of_species[i]\n",
    "    \n",
    "    \n",
    "    \n",
    "    query = make_pubmed_advance_search_query(species_name)\n",
    "    abstract = get_abstract_from_pubmed(query)\n",
    "    \n",
    "\n",
    "    if abstract is None:                  #this occurs when there is no search result for a query\n",
    "        list_of_species_with_no_record.append(species_name)\n",
    "        pH_sentence = '---'\n",
    "        what_regexss_found_pH = '---'\n",
    "        the_sentence_about_temerature = '---'\n",
    "        what_regexss_found_temperature = '---'\n",
    "        \n",
    "\n",
    "    if abstract is not None:\n",
    "        abstract = get_abstract_make_changes(abstract)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pH_sentence = find_sentence_with_pH_data(abstract)\n",
    "        the_sentence_about_temerature = find_sentence_with_temerature_data(abstract)\n",
    "        what_regexss_found_pH = get_sentence_give_pH_data(pH_sentence)\n",
    "        what_regexss_found_temperature = get_sentence_give_temperature_data(the_sentence_about_temerature)\n",
    "\n",
    "\n",
    "\n",
    "    #print (species_name , \"===================\" , query , \"++++++++++++++++++++\", abstract , \"******************\" , pH_sentence , \">>>>>>>>>>>>>>>\" , what_regex_found)\n",
    "    #print (pH_sentence , \">>>>>>>>>>>>>>>\" , what_regexss_found)\n",
    "\n",
    "    #making lists and finally a dataframe\n",
    "    list_data = [ID, species_name , query , abstract , pH_sentence ,  what_regexss_found_pH,the_sentence_about_temerature, what_regexss_found_temperature]\n",
    "    while len (list_data) != 8:\n",
    "        list_data.append(' ')\n",
    "    #print(list_data)\n",
    "\n",
    "    data_series = pd.Series(list_data,index = df.columns)\n",
    "    df = df.append(data_series, ignore_index=True) \n",
    "\n",
    "#we dont need this output\n",
    "#df.to_csv(r'C:\\Users\\kamy\\Desktop\\final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi! , using PubMed API we found 27 species with some records and we extracted the data we need , but there is still  67 species without any records\n"
     ]
    }
   ],
   "source": [
    "print('hi! , using PubMed API we found' , len(list_of_species)-len(list_of_species_with_no_record) , 'species with some records and we extracted the data we need , but there is still ', str(len(list_of_species_with_no_record)) , 'species without any records')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all the data together\n",
    "all_data_together = pd.concat([result, df], axis=1)\n",
    "######################################################\n",
    "# save the output\n",
    "all_data_together.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP SEVEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what are the availible seq data according to BacDive????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_together = pd.read_csv(r'C:\\Users\\kamy\\Desktop\\OUTPUT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making some links to extract all availible data from BacDive\n",
    "\n",
    "link_part_one = r'https://bacdive.dsmz.de/strain/'\n",
    "links_list = []\n",
    "for i in all_data_together['ID']:\n",
    "    link = link_part_one + str(i)\n",
    "    links_list.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting from BacDive\n",
    "\n",
    "all_urls = links_list\n",
    "\n",
    "def read_html(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    return None\n",
    "    #CODE = 200 means the url is availible\n",
    "\n",
    "\n",
    "\n",
    "my_data_frame = pd.DataFrame()\n",
    "COUNTER = 0\n",
    "for url in all_urls:\n",
    "    COUNTER = COUNTER +1\n",
    "    html_doc = read_html(url)\n",
    "    if html_doc is None:\n",
    "        print(\"Something went wrong!!!  the following url seems to be wrong   ; \" , url)\n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "\n",
    "    listm = [url]\n",
    "\n",
    "    #first step ==> extracting seq data\n",
    "    tag = \"valigntop border padding\"\n",
    "    data = soup.find_all(\"td\", class_= tag)\n",
    "    for td in data:\n",
    "        listm.append(td.text)\n",
    "    # some of the lists contains more than 20 availible seq\n",
    "    ########################### THIS IS MY HYPER-PARAMETER ##############################\n",
    "    maximum_lenght_list = 300\n",
    "    while len(listm) < maximum_lenght_list:\n",
    "        listm.append('')\n",
    "\n",
    "\n",
    "    my_data_frame[COUNTER] = pd.Series(listm)\n",
    "    \n",
    "\n",
    "my_data_frame = my_data_frame.T\n",
    "\n",
    "#my_data_frame.to_csv(r\"C:\\Users\\kamy\\Desktop\\all_availible_seq_not_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code seperates the sequence colums\n",
    "\n",
    "null = ''\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in range(0,len(my_data_frame)):\n",
    "    this_row = []\n",
    "    for j in range (0, maximum_lenght_list):\n",
    "        \n",
    "        if \"tax ID\" not in str(my_data_frame.iloc[i][j]):\n",
    "            this_row.append(my_data_frame.iloc[i][j])\n",
    "            \n",
    "        if \"tax ID\" in str(my_data_frame.iloc[i][j]):\n",
    "            while len(this_row)%7 != 0:\n",
    "                this_row.append(null)\n",
    "            this_row.append(my_data_frame.iloc[i][j])\n",
    "\n",
    "        \n",
    "        \n",
    "########################### THIS IS MY HYPER-PARAMETER ##############################\n",
    "    maximum_lenght_this_row = 400\n",
    "    while len(this_row) != maximum_lenght_this_row:\n",
    "        this_row.append(null)\n",
    "     \n",
    "    df[i] = pd.Series(this_row)\n",
    "            \n",
    "df = df.T\n",
    "\n",
    "\n",
    "# this code relocate the tax data\n",
    "for i in range(0,len(df)):\n",
    "    for j in range (0,maximum_lenght_this_row):\n",
    "        if 'tax' in str(df.iloc[i][j]):\n",
    "            df.iloc[i][j-2] = df.iloc[i][j]\n",
    "            df.iloc[i][j] = ''\n",
    "                    \n",
    "                    \n",
    "#df.to_csv(r\"C:\\Users\\kamy\\Desktop\\all_availible_seq_clean.csv\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP EIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_accession_numbers = []\n",
    "for counter in range (2,300,7):\n",
    "    column_of_accessions_in_df = df[counter]\n",
    "\n",
    "    for accession_number in column_of_accessions_in_df:\n",
    "        if accession_number != '':\n",
    "            list_of_accession_numbers.append(accession_number)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_accession_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['KT935587',\n",
       " 'KP233895',\n",
       " 'AY959944',\n",
       " 'AB186359',\n",
       " 'AB125279',\n",
       " 'KJ174056',\n",
       " 'GCA_000196455',\n",
       " 'AJ621845',\n",
       " 'FM178869',\n",
       " 'HQ398213',\n",
       " 'X96960',\n",
       " 'X96956',\n",
       " 'AY744449',\n",
       " 'X96957',\n",
       " 'X96958',\n",
       " 'DQ834685',\n",
       " 'AJ297449',\n",
       " 'DQ834681',\n",
       " 'AB611036',\n",
       " 'L37422',\n",
       " 'AB910748',\n",
       " 'GCA_902860145',\n",
       " 'M22509',\n",
       " 'HG324052',\n",
       " 'GCA_003315095',\n",
       " 'AB010840',\n",
       " 'HM219615',\n",
       " 'KC771236',\n",
       " 'AF016691',\n",
       " 'X90484',\n",
       " 'X90477',\n",
       " 'X89852',\n",
       " 'JQ513288',\n",
       " 'KJ921703',\n",
       " 'AY907891',\n",
       " 'JX412366',\n",
       " 'AY140238',\n",
       " 'KF017280',\n",
       " 'LC027464',\n",
       " 'FR774763',\n",
       " 'KC662255',\n",
       " 'KC128649',\n",
       " 'AJ459800',\n",
       " 'EU653290',\n",
       " 'KU213664',\n",
       " 'AY034139',\n",
       " 'AF191225',\n",
       " 'AY350586',\n",
       " 'U75647',\n",
       " 'M79366',\n",
       " 'NR_074327',\n",
       " 'KM083127',\n",
       " 'AB561884',\n",
       " 'AM943980',\n",
       " 'AY907888',\n",
       " 'JQ283461',\n",
       " 'JQ283460',\n",
       " 'AM947652',\n",
       " 'D86512',\n",
       " 'AB517669',\n",
       " 'KF537578',\n",
       " 'FN686779',\n",
       " 'KR905751',\n",
       " 'NR_074660',\n",
       " 'AF376020',\n",
       " 'Y11595',\n",
       " 'HM070044',\n",
       " 'FN686787',\n",
       " 'AM296495',\n",
       " 'AB189141',\n",
       " 'KX100033',\n",
       " 'FJ870383',\n",
       " 'FJ870384',\n",
       " 'KX306477',\n",
       " 'GCA_003815035',\n",
       " 'GCA_008125075',\n",
       " 'GCA_000237085',\n",
       " '499177.3',\n",
       " 'AB665077',\n",
       " 'AB665079',\n",
       " 'JF793961',\n",
       " 'GCA_001263355',\n",
       " 'GCA_008086595',\n",
       " 'X96954',\n",
       " 'GCA_004340205',\n",
       " 'X89077',\n",
       " 'AJ243189',\n",
       " 'JQ746422',\n",
       " 'GCA_902859595',\n",
       " 'GCA_902859635',\n",
       " 'JQ746426',\n",
       " 'GCA_900103005',\n",
       " 'D85506',\n",
       " 'D26489',\n",
       " 'D85505',\n",
       " 'GCA_013133895',\n",
       " '619593.4',\n",
       " 'GCA_000759655',\n",
       " '1281578.45',\n",
       " 'M79416',\n",
       " 'EU653291',\n",
       " 'GCA_000023265',\n",
       " 'M79367',\n",
       " '62140.4',\n",
       " 'GCA_004123295',\n",
       " 'GCA_004339725',\n",
       " 'GCA_001402945',\n",
       " 'GCA_001402935',\n",
       " 'GCA_001311945',\n",
       " 'AJ459804',\n",
       " 'AJ278719',\n",
       " 'GCA_900174455',\n",
       " 'X75267',\n",
       " 'AF512812',\n",
       " 'GCA_000688455',\n",
       " '1817405.3',\n",
       " '320502.3',\n",
       " 'GCA_007827345',\n",
       " 'D30768',\n",
       " 'AJ419837',\n",
       " 'AY829472',\n",
       " 'GCA_008086625',\n",
       " '52693.4',\n",
       " 'GCA_000247605',\n",
       " '979970.3',\n",
       " 'GCA_000144695',\n",
       " 'GCA_000266925',\n",
       " 'AJ278451',\n",
       " '470868.5',\n",
       " 'GCA_002082135',\n",
       " '1120920.3',\n",
       " 'GCA_009729015',\n",
       " 'GCA_003201835',\n",
       " 'GCA_009729545',\n",
       " '209650.4',\n",
       " 'M79417',\n",
       " 'GCA_001753245',\n",
       " '525909.11',\n",
       " 'M79368',\n",
       " '1560006.3',\n",
       " '768535.4',\n",
       " 'GCA_001399695',\n",
       " 'GCA_001316225',\n",
       " '1294026.5',\n",
       " 'GCA_001931655',\n",
       " 'GCA_003966655',\n",
       " 'GCA_000750615',\n",
       " 'AJ459803',\n",
       " '1382359.3',\n",
       " '2786546168',\n",
       " '2509887034',\n",
       " '720554.3',\n",
       " 'JF793947',\n",
       " 'AB680507',\n",
       " 'GCA_007991395',\n",
       " 'GCA_900235925',\n",
       " '931626.3',\n",
       " '2731639222',\n",
       " '574087.3',\n",
       " '891968.3',\n",
       " 'Y14907',\n",
       " '2770939588',\n",
       " 'GCA_902859695',\n",
       " '2599185234',\n",
       " 'GCA_009428885',\n",
       " '41673.4',\n",
       " '12915.4',\n",
       " 'AF387301',\n",
       " 'GCA_001753165',\n",
       " '644736322',\n",
       " 'Y18445',\n",
       " '2757320406',\n",
       " '507754.4',\n",
       " '1295373.3',\n",
       " '119978.4',\n",
       " '1232575.4',\n",
       " 'GCA_001857665',\n",
       " 'AY552087',\n",
       " '2558860986',\n",
       " '288965.3',\n",
       " 'JF793949',\n",
       " 'AB032351',\n",
       " 'GCA_011516825',\n",
       " 'GCA_900562175',\n",
       " '648028002',\n",
       " '2509601011',\n",
       " 'AB680772',\n",
       " 'GCA_002803005',\n",
       " '2283.10',\n",
       " 'GCA_003333315',\n",
       " 'GCA_000754095',\n",
       " 'D30773',\n",
       " '507754.5',\n",
       " '312540.4',\n",
       " '1232575.7',\n",
       " 'GCA_001685225',\n",
       " 'EF059790',\n",
       " '2507262051',\n",
       " 'AB111902',\n",
       " 'GCA_011516875',\n",
       " '304077.7',\n",
       " '52689.8',\n",
       " 'GCA_001514355',\n",
       " 'GCA_002799605',\n",
       " '2283.11',\n",
       " 'GCA_001705075',\n",
       " '160660.10',\n",
       " 'GCA_002255305',\n",
       " 'M79397',\n",
       " '2596583699',\n",
       " 'AB680674',\n",
       " '2861688124',\n",
       " '2829864053',\n",
       " '52689.9',\n",
       " 'GCA_013267395',\n",
       " 'GCA_001637085',\n",
       " '2283.4',\n",
       " '163359.11',\n",
       " '160660.4',\n",
       " '160808.24',\n",
       " 'M79398',\n",
       " 'AB003967',\n",
       " '52689.10',\n",
       " 'GCA_003812265',\n",
       " 'GCA_001637115',\n",
       " '163359.15',\n",
       " '160808.5',\n",
       " 'Y11596',\n",
       " 'X74066',\n",
       " '52689.11',\n",
       " 'GCA_002205315',\n",
       " 'GCA_001637095',\n",
       " '160808.28',\n",
       " 'GCA_000227215',\n",
       " 'GCA_004341595',\n",
       " 'GCA_013343095',\n",
       " 'GCA_001637105',\n",
       " '160808.19',\n",
       " 'GCA_009662475',\n",
       " '887700.10',\n",
       " 'GCA_013267375',\n",
       " 'GCA_902860195',\n",
       " '160808.29',\n",
       " 'GCA_006718285',\n",
       " '2784746776',\n",
       " 'GCA_900444675',\n",
       " 'GCA_012956685',\n",
       " '160808.20',\n",
       " 'GCA_001705805',\n",
       " 'GCA_902859715',\n",
       " '72557.10',\n",
       " '160808.30',\n",
       " 'GCA_001705695',\n",
       " 'GCA_003939895',\n",
       " '72557.13',\n",
       " '160808.18',\n",
       " 'GCA_001756595',\n",
       " 'GCA_001945385',\n",
       " '72557.12',\n",
       " 'GCA_001705625',\n",
       " 'GCA_002192685',\n",
       " '72557.11',\n",
       " 'GCA_001705645',\n",
       " 'GCA_002338195',\n",
       " '72557.39',\n",
       " 'GCA_000709715',\n",
       " '32002.33',\n",
       " '72557.40',\n",
       " 'GCA_001705755',\n",
       " '32002.23',\n",
       " '72557.38',\n",
       " 'GCA_001705725',\n",
       " '32002.16',\n",
       " 'GCA_002079865',\n",
       " '32002.22',\n",
       " '637390.5',\n",
       " '32002.15',\n",
       " '637390.19',\n",
       " '32002.30',\n",
       " '637390.18',\n",
       " '32002.17',\n",
       " '930.14',\n",
       " '32002.32',\n",
       " '930.11',\n",
       " '32002.19',\n",
       " '930.22',\n",
       " '32002.4',\n",
       " '930.8',\n",
       " '930.15',\n",
       " '930.10',\n",
       " '930.12',\n",
       " '930.4',\n",
       " '930.17']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_accession_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now its time to extract the subsequent sequence data using their accession numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "erorrs = []\n",
    "list_id = []\n",
    "list_seq =[]\n",
    "list_description =[]\n",
    "for i in list_of_accession_numbers:\n",
    "    try:\n",
    "        handle = Entrez.efetch(db = \"nucleotide\", id = i, rettype = \"fasta\")\n",
    "        record = SeqIO.read( handle, \"fasta\" )\n",
    "        list_id.append(record.id)\n",
    "        list_description.append(record.description)\n",
    "        list_seq.append(record.seq)\n",
    "        \n",
    "    except:\n",
    "        erorrs.append(i)\n",
    "        \n",
    "list_of_tuples = list(zip(list_description, list_seq))\n",
    "df_seq = pd.DataFrame(list_of_tuples,columns = [\"description\",'sequence'])\n",
    "df_seq.to_csv(r\"C:\\Users\\kamy\\Desktop\\16S_seq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      KT935587.1 Abyssicoccus albus strain LIPI11-2-...\n",
       "1      KP233895.1 Abyssivirga alkaniphila strain L81 ...\n",
       "2      AY959944.2 Clostridium alkalicellum 16S riboso...\n",
       "3      AB186359.1 Clostridium clariflavum gene for 16...\n",
       "4      AB125279.1 Clostridium straminisolvens gene fo...\n",
       "                             ...                        \n",
       "110    AB680674.1 Acetobacter aceti gene for 16S rRNA...\n",
       "111    M79398.1 Thiobacillus thiooxidans 16S rRNA seq...\n",
       "112    AB003967.1 Acetobacter aceti 16S rRNA, partial...\n",
       "113    Y11596.1 Acidithiobacillus thiooxidans 16S rRN...\n",
       "114                   X74066.1 A.aceti gene for 16S rRNA\n",
       "Name: description, Length: 115, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_seq[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(0,len(df)):\n",
    "    for counter in range (2,300,7):\n",
    "        cell_of_accessions_in_df = df.iloc[row][counter]\n",
    "        for second_counter in range (0,len(df_seq)):\n",
    "            if cell_of_accessions_in_df in df_seq[\"description\"][second_counter]:\n",
    "                df.iloc[row][counter+4] = df_seq[\"description\"][second_counter]\n",
    "                df.iloc[row][counter+5] = df_seq[\"sequence\"][second_counter]\n",
    "\n",
    "                \n",
    "df.to_csv(r\"C:\\Users\\kamy\\Desktop\\all_seq.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
